\documentclass{hw}

\title{MATH 168, Assignment 1}
\author{Phil Chodrow}

\begin{document}

\problem{}

You're likely familiar with the algebraic definition of the eigenvalues and eigenvectors of a matrix $\mathbf{A}$ as solutions to the equation $\mathbf{A}\mathbf{v} = \lambda \mathbf{v}$. 
In this problem, we will prove an additional viewpoint on eigenvalues and eigenvectors that is especially important in network science and many other fields of applied mathematics: 
\begin{quote}
    \emph{For symmetric matrices, eigenvalues/eigenvectors are solutions of optimization problems.} 
\end{quote}
Let $\lVert\mathbf{v} \rVert$ be the Euclidean norm of a vector $\mathbf{v}$. 
Here's our theorem: 
\begin{thm}\label{thm:variational}
    Let $\mathbf{A} \in \mathbb{R}^{n\times n}$ be a symmetric matrix. 
    Let $\lambda_1,\ldots,\lambda_n$ be the eigenvalues of $\mathbf{A}$ ordered from smallest ($\lambda_1$) to largest ($\lambda_n$).  
    Then, 
    \begin{align*}
        \lambda_n = \max_{\mathbf{v} \in \mathbb{R}^n} \left\{\mathbf{v}^T\mathbf{A}\mathbf{v} \;\big|\; \lVert{\mathbf{v}}\rVert = 1\right\} \;.
    \end{align*}
\end{thm}
In words, the largest value of the function $f(\mathbf{v}) = \mathbf{v}^T\mathbf{A}\mathbf{v}$ that can be obtained by picking a vector $\mathbf{v} \in \mathbb{R}^n$ satisfying $\lVert \mathbf{v} \rVert = 1$ is $\lambda_n$. 
The reason that \Cref{thm:variational} matters from an applied standpoint is that many network science and data analysis tasks correspond to maximizing functions that look like $f(\mathbf{v}) = \mathbf{v}^T\mathbf{A}\mathbf{v}$.  


\part

    Prove \Cref{thm:variational} in three steps. 
    \begin{enumerate}
        \item Expand $\mathbf{A}$ in an orthonormal basis of eigenvectors: 
        \begin{align*}
            \mathbf{A} = \sum_{i = 1}^n \lambda_i\mathbf{u}_i\mathbf{u}_i^T\;,
        \end{align*}
        where each $\mathbf{u}_i$ is an eigenvector $\mathbf{A}$ with eigenvalue $\lambda_i$. 
        As a reminder, orthonormality means that $\lVert \mathbf{u}_i \rVert = 1$ and $\mathbf{u}_i^T \mathbf{u}_j = 1$ if $i = j$ and $0$ otherwise.   
        Explain carefully why you are allowed to perform this expansion (cite a theorem). 
        \item Write $\mathbf{v} = \sum_{i = 1}^n \alpha_i \mathbf{u}_i$ for some undetermined coefficients $\{\alpha_i\}$, and explain why you are allowed to do this. 
        Compute $\lVert \mathbf{v}\rVert$ in terms of the coefficients $\{\alpha_i\}$, and determine what the constraint $\lVert \mathbf{v} \rVert = 1$ implies about $\{\alpha_i\}$. 
        \item Finally, use the first two steps to compute $\mathbf{v}^T\mathbf{A}\mathbf{v}$ directly in terms of $\{\lambda_i\}$ and $\{\alpha_i\}$. 
        Show that the result is maximized when $\alpha_n = 1$ and $\alpha_i = 0$ for $i < n$. 
        Include a closing sentence to help your reader connect this final result to \Cref{thm:variational}. 
    \end{enumerate}
    
    \solution

\part 

    Prove \Cref{thm:variational} (again) using the \emph{principle of Lagrange multipliers}. 
    The principle of Lagrange multipliers states that, if $\mathbf{x}$ solves the problem 
    \begin{align*}
        \max \left\{f(\mathbf{x}) \;\big|\; g(\mathbf{x}) = 0\right\} \;,
    \end{align*}
    then $\mathbf{x}$ must also satisfy the equation 
    \begin{align*}
        \nabla f(\mathbf{x}) + c \nabla g(\mathbf{x}) = 0
    \end{align*}
    for some scalar $c \in \mathbb{R}$, provided that $f$ and $g$ satisfy certain technical conditions (which you may assume are satisfied here). 
    
    You may assume without proof that any continuous function on the unit sphere $S^{n-1} = \{\mathbf{v} : \lVert \mathbf{v}\rVert = 1\}$ possesses at least one maximum value (this follows from the compactness of $S^{n-1}$). 
    You may also assume without proof that the function $f(\mathbf{v}) = \mathbf{v}^T\mathbf{A}\mathbf{v}$ is continuous. 
    State clearly where you use these facts in your proof. 
    Please again make sure to explicitly state where you use the hypothesis that $\mathbf{A}$ is symmetric. 

    \solution

    \part 
    
    Using \Cref{thm:variational}, give an \emph{extremely short} proof that
    \begin{align*}
        \lambda_1 = \min_{\mathbf{v} \in \mathbb{R}^n} \left\{\mathbf{v}^T\mathbf{A}\mathbf{v} \;\big|\; \lVert{\mathbf{v}}\rVert = 1\right\} \;.
    \end{align*} 

    \solution 

\problem{}

    Perron-Frobenius problem here. 


    \solution

\problem{}

    

    Let $X_1,X_2\ldots,$ be an infinite sequence of i.i.d. random variables. 
    Let $N$ be a strictly positive random variable that is independent of the sequence $\{X_1,X_2,\ldots\}$. 
    Let 
    \begin{align*}
        Y = \sum_{i = 1}^N X_i\;. 
    \end{align*}
    Prove that $\mathbb{E}[Y] = \mathbb{E}[X_1]\mathbb{E}[N]$. 

    \solution

\problem{}

    In this problem, you will argue that a binomial distribution can be approximated by a Poisson distribution in a certain limit. 
    This approximation will be useful when we study properties of random graph models. 
    
    As a reminder, a nonnegative discrete random variable $X$ has a \emph{binomial distribution} with number of trials $n$ and success rate $p$ if its probability mass function is 
    \begin{align*}
        p_X(k) = \binom{n}{k}p^{k}(1-p)^{n-k}\;. 
    \end{align*}
    A nonnegative discrete random variable $Y$ has a \emph{Poisson distribution} with parameter $\mu$ if its probability mass function is 
    \begin{align*}
        p_Y(k) = \frac{\mu^ke^{-\mu}}{k!}\;. 
    \end{align*}
    Suppose now that $X$ is binomial and that $p = c / n$. 
    Show that, as $n$ grows large, $X$ is approximately Poisson. 
    Identify the parameter of the Poisson distribution in terms of $c$ and $n$. 
    It suffices to show that the PMF of $X$ converges to a Poisson PMF (formally, we are showing \emph{convergence in distribution}). 

    The following approximations are valid for large $n$, when $k$ and $m$ are fixed constants.  
    You may use them without proof. 
    \begin{align*}
        \binom{n}{k} \simeq \frac{n^k}{k!} \quad \text{and} \quad 
        e^{m}      \simeq \left(1 + \frac{m}{n}\right)^{n}\;.        
    \end{align*} 

    \solution

\problem{}

Please briefly respond to the following questions: 
\begin{enumerate}
    \item Why are you interested in learning about networks? 
    \item What is one question you have about the role that networks play in your life today?
    \item What is something related to networks that you might like to achieve by the end of the quarter? 
    \item MATH 168 has upper-div prerequisites in linear algebra and probability. 
    Pick one of these two prerequisites. 
    Share something that you thought was especially cool or interesting, and something that you found especially disappointing or frustrating. 
\end{enumerate}
A few sentences for each item is plenty. 


\end{document}